{
    "dataset": "leverco_shortened",
    "run": "e2fce76f-06c0-4367-ada7-6723970520b4",
    "source": "",
    "snippets_used": 3,
    "attempts": 9,
    "final_scraper_code": "# imports\nimport bs4\nimport requests\nimport json\n\ndef scraper(url: str):\n  # add headers to simulate a real browser\n  headers = {'User-Agent': 'Mozilla/5.0'}\n\n  # fetch website\n  response = requests.get(url, headers=headers)\n\n  if response.status_code == 200:\n    # parse HTML\n    soup = bs4.BeautifulSoup(response.text, 'html.parser')\n\n    # find all job listings\n    jobs = soup.find_all('div', class_='posting')\n\n    # create empty list to store data\n    data = []\n\n    # loop through job listings\n    for job in jobs:\n      # extract title, tag, apply link\n      title = job.find('h5', attrs={'data-qa': 'posting-name'}).text.strip()\n      tag = job.find('span', class_='display-inline-block').text\n      apply_link = job.find('a', class_='posting-btn-submit')['href']\n\n      # create dictionary with data\n      job_data = {'title': title, 'tag': tag, 'apply_link': apply_link}\n\n      # append to list\n      data.append(job_data)\n\n    # print data as JSON\n    print(json.dumps(data, indent=2))\n    \n  else:\n    print(\"Could not fetch website. Error code:\", response.status_code)\n\nif __name__ == '__main__':\n  url = 'https://jobs.lever.co/appboxo'\n  scraper(url)",
    "test_count": 8,
    "test_succ_count": 8
}